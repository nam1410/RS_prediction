# A Closer Look at Weak Supervision’s Limitations in WSI Recurrence Score Prediction

### Summary

<img src="9.png" width="450" height="450" align="left">

Histological examination and derived ancillary testing remain the gold standard for breast cancer diagnosis, prognosis assessment and treatment guidance. Currently, a commercial molecular signature test ONCOTYPEDX®, based on RNA quantitation and providing a recurrence score (RS) ranging from 0 to 100, is routinely utilized to predict the probabilities of response to chemotherapy and disease recurrence. We attempted to predict RS using digital pathology and weakly supervised AI models. In tissue samples, the malignant component is haphazardly admixed with the non-malignant component in variable proportions. This represents a challenge for weakly supervised attention-based models to identify high-valued diagnostic/prognostic areas within whole slide images (WSIs). 

After training traditional convolutional ([CLAM](https://github.com/mahmoodlab/CLAM)) and transformer ([TransMIL](https://github.com/szc19990412/TransMIL)) models on our in-house dataset, they exhibited a counterintuitive behaviour by emphasizing non-malignant features while neglecting the essential discriminative characteristics associated with tumoural regions and eventually misinterpretation. The visual representation in Fig illustrates issues with attention weights encountered in our initial attempt to classify <i>high</i> vs <i>low</i> RS. In <b>Fig A</b>, the heatmap for a WSI incorrectly classified as belonging to class <i>high</i> (actually <i>low</i>) displays a <i>negative picture</i> of attention (paying attention to non-tumoural component), understandably arising from the incorrect classification. Conversely, <b>Fig B</b> demonstrates the scenario where the WSI is correctly classified, yet the heatmap generated is also another <i>negative picture</i> of attention and unrelated to the class assignment. Numerous false-positive and false-negative errors become apparent, signalling the persistence of non-malignant features originating from non-tumoural areas or artifacts. 

To address this, we propose an interactive approach with a human in the middle by creating a user-friendly Graphical User Interface (GUI) that allows an expert pathologist to annotate heatmaps generated by any attention-based model. We aim to enhance the model’s learning capabilities and performance by incorporating the feedback from the GUI as expected scores in the successive training process. We train [CLAM](https://github.com/mahmoodlab/CLAM) and [TransMIL](https://github.com/szc19990412/TransMIL) on our in-house dataset before and after the expert feedback. We observe an improvement in RS prediction after retraining both models with the pathologist’s annotation- a 5% rise in validation-test AUC and 4% in validation-test accuracy for CLAM and a 4.5% increase in validation-test AUC and 3% in validation-test accuracy for TransMIL. 

Furthermore, we analyzed the generated heatmaps and observed how additional supervision from a domain expert enhanced the learning capacity of the models. We notice an improvement in cosine similarity between the pathologist’s GUI-based attention scores and trained models’ attention maps after feedback - 5% and 10% increase for CLAM and TransMIL, respectively. Our adaptive, interactive system harmonizes attention scores with expert intuition and instills higher confidence in the system’s predictions. This study establishes a potent synergy between AI and expert collaboration, addressing the constraints of weak supervision by enhancing the discrimination of diagnostic features and making an effort to generate predictions according to clinical diagnostic norms.

### Installation Guide

We perform experiments on a consumer-grade workstation equipped with the NVIDIA GeForce 3090 GPU. 

Create a [virtual environment](https://docs.python.org/3/library/venv.html) and run the following:

```
pip install -r requirements.txt
```

### Pre-processing

We follow pre-processing from [CLAM](https://github.com/mahmoodlab/CLAM)
```
#segmentation and create patches
'''
convert the images to HSV colour space and generate binary masks for tissue regions using thresholding of the saturation channel
extract 256 × 256 patches from the segmented foreground contours and store their coordinates
'''
python create_patches_fp.py --source DATA_DIRECTORY --save_dir RESULTS_DIRECTORY --patch_size 256 --seg --process_list CSV_FILE_NAME --patch --stitch

#extract features
'''
employ ImageNet pretrained ResNet50 that converts each patch into a 1024-dimensional feature vector representation
extracted features serve as inputs during the training
'''
CUDA_VISIBLE_DEVICES=0,1 python extract_features_fp.py --data_h5_dir DIR_TO_COORDS --data_slide_dir DATA_DIRECTORY --csv_path CSV_FILE_NAME --feat_dir FEATURES_DIRECTORY --batch_size 512 --slide_ext .svs

#create splits
python create_splits_seq.py --task task_2_tumor_subtyping  --k 10
```


### Details

1. Run the vanilla version of CLAM
   ```
   python main_vanilla_CLAM.py --drop_out --early_stopping --lr 2e-4 --k 10 --label_frac 1.0 --exp_code task_2_tumor_subtyping_CLAM_50 --weighted_sample --bag_loss ce --inst_loss svm --task task_2_tumor_subtyping --model_type clam_mb --log_data --data_root_dir <base-data-dir>
   ```

2. Generate heatmaps using [create_heatmaps.py](https://github.com/mahmoodlab/CLAM/blob/master/create_heatmaps.py) from the CLAM repository. In addition, generate the reverse heatmaps (reversing the attention scores) using [create_heatmaps.py](https://github.com/mahmoodlab/CLAM/blob/master/create_heatmaps.py) from the CLAM repository with a small change [here](https://github.com/mahmoodlab/CLAM/blob/9482cbc72df522087cfbaa3e6b52da5207a7980a/wsi_core/WholeSlideImage.py#L623):
   ```
   if isinstance(cmap, str):
            cmap = plt.get_cmap(cmap)
            cmap = cmap.reversed()
   ```

4. Once both the versions of heatmaps are generated, save them in a CSV format. For instance, ```<your-heatmaps-csv-filename>.csv``` can be:
   
   | <b>name</b> | <b>webContentLink</b>   | 
   | :---:   | :---: | 
   | slide_0 | link-of-heatmap-of-slide_0 |
   | slide_1 | link-of-heatmap-of-slide_1 |
   | : | : |
   | slide_n | link-of-heatmap-of-slide_n |

   ```<your-reverse-heatmaps-csv-filename>.csv``` can be:

    | <b>name</b> | <b>webContentLink</b>   | 
   | :---:   | :---: | 
   | slide_0 | link-of-reverse-heatmap-of-slide_0 |
   | slide_1 | link-of-reverse-heatmap-of-slide_1 |
   | : | : |
   | slide_n | link-of-reverse-heatmap-of-slide_n |

5. To use the GUI, run
   ```
   #change the PWD to Heatmap_GUI/
   cd Heatmap_GUI

   #run the python script
   python deep_trial.py

   * Serving Flask app 'deep_trial'
   * Debug mode: off
   * Running on http://xxx.x.x.x:xxxx
   #local host address is not xxx.x.x.x:xxxx, it is subjective to your machine's sockets, ports and addresses
   ```
   Copy the address to any of your web browser and select any slide to annotate.



   

   
